"""
Scraping est√°tico con requests + BeautifulSoup
"""
import requests
from bs4 import BeautifulSoup
from typing import Tuple, List, Optional
import logging
import time

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def fetch_html(url: str, timeout: int = 10) -> Optional[str]:
    """
    Descarga el HTML de una URL usando requests.
    
    Args:
        url: URL a descargar
        timeout: Timeout en segundos
        
    Returns:
        HTML como string o None si falla
    """
    headers = {
        'User-Agent': 'BrochureAI/0.1 (Educational Project; contact@example.com)',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'es-ES,es;q=0.9,en;q=0.8',
    }
    
    try:
        logger.info(f"Descargando: {url}")
        response = requests.get(url, headers=headers, timeout=timeout)
        response.raise_for_status()  # Lanza excepci√≥n si status >= 400
        
        logger.info(f"‚úì Descargado {len(response.text)} caracteres (status {response.status_code})")
        return response.text
        
    except requests.exceptions.Timeout:
        logger.error(f"‚úó Timeout alcanzado para {url}")
        return None
        
    except requests.exceptions.HTTPError as e:
        logger.error(f"‚úó Error HTTP {e.response.status_code}: {url}")
        return None
        
    except requests.exceptions.RequestException as e:
        logger.error(f"‚úó Error de conexi√≥n: {e}")
        return None


def extract_links(html: str, base_url: str) -> List[str]:
    """
    Extrae todos los enlaces <a href> del HTML.
    Ignora scripts, estilos, im√°genes, inputs.
    
    Args:
        html: HTML como string
        base_url: URL base para debugging (no normaliza aqu√≠)
        
    Returns:
        Lista de URLs extra√≠das (pueden ser relativas o absolutas)
    """
    soup = BeautifulSoup(html, 'lxml')
    
    # Eliminar elementos irrelevantes ANTES de buscar enlaces
    for tag in soup(['script', 'style', 'noscript', 'iframe']):
        tag.decompose()
    
    links = []
    
    for a_tag in soup.find_all('a', href=True):
        href = a_tag['href'].strip()
        
        # Filtros b√°sicos
        if not href:
            continue
        if href.startswith('#'):  # Anclas internas
            continue
        if href.startswith('mailto:'):
            continue
        if href.startswith('tel:'):
            continue
        if href.startswith('javascript:'):
            continue
            
        links.append(href)
    
    logger.info(f"Extra√≠dos {len(links)} enlaces del HTML")
    return links


def scrape_and_extract(url: str) -> Tuple[Optional[str], List[str]]:
    """
    Funci√≥n principal: descarga HTML y extrae enlaces.
    
    Args:
        url: URL a scrapear
        
    Returns:
        (html, lista_de_enlaces)
    """
    html = fetch_html(url)
    
    if html is None:
        logger.warning(f"No se pudo descargar {url}")
        return None, []
    
    links = extract_links(html, url)
    
    return html, links


"""
A√±adir al final de src/scraping.py
"""
from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeout
import os
from dotenv import load_dotenv

load_dotenv()

# Rate limiting
RATE_LIMIT_DELAY = float(os.getenv('RATE_LIMIT_DELAY', '1.5'))


def is_incomplete_html(html: str, url: str) -> bool:
    """
    Detecta si el HTML parece incompleto (requiere JS para renderizar).
    
    Heur√≠sticas:
    1. HTML muy corto (<1500 chars) pero es un sitio complejo
    2. Tiene <div id="root"> o <div id="app"> vac√≠os (React/Vue)
    3. Tiene textos como "Loading...", "Please enable JavaScript"
    4. Ratio script/contenido muy alto
    
    Args:
        html: HTML descargado
        url: URL para contexto
        
    Returns:
        True si parece incompleto
    """
    soup = BeautifulSoup(html, 'lxml')
    
    # Heur√≠stica 1: HTML muy corto
    if len(html) < 1500:
        logger.warning(f"HTML muy corto ({len(html)} chars) - posible SPA")
        return True
    
    # Heur√≠stica 2: Divs t√≠picos de SPAs vac√≠os
    root_div = soup.find('div', id='root')
    app_div = soup.find('div', id='app')
    
    if root_div and len(root_div.get_text(strip=True)) < 50:
        logger.warning("Detectado <div id='root'> casi vac√≠o - SPA React")
        return True
        
    if app_div and len(app_div.get_text(strip=True)) < 50:
        logger.warning("Detectado <div id='app'> casi vac√≠o - SPA Vue")
        return True
    
    # Heur√≠stica 3: Textos indicadores de JS requerido
    text_content = soup.get_text().lower()
    js_indicators = [
        'please enable javascript',
        'requires javascript',
        'javascript is disabled',
        'loading...',
        'cargando...'
    ]
    
    for indicator in js_indicators:
        if indicator in text_content:
            logger.warning(f"Detectado texto '{indicator}' - requiere JS")
            return True
    
    # Heur√≠stica 4: Pocos enlaces extra√≠dos (pero muchos scripts)
    links = soup.find_all('a', href=True)
    scripts = soup.find_all('script')
    
    if len(links) < 5 and len(scripts) > 10:
        logger.warning(f"Pocos enlaces ({len(links)}) pero muchos scripts ({len(scripts)}) - posible SPA")
        return True
    
    logger.info("HTML parece completo - scraping est√°tico OK")
    return False


def fetch_html_dynamic(url: str, timeout: int = 30000) -> Optional[str]:
    """
    Descarga HTML usando Playwright (navegador headless).
    Espera a que la p√°gina cargue completamente.
    
    Args:
        url: URL a renderizar
        timeout: Timeout en milisegundos (default 30s)
        
    Returns:
        HTML renderizado o None si falla
    """
    try:
        logger.info(f"üåê Usando navegador headless para: {url}")
        
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            page = browser.new_page(
                user_agent='BrochureAI/0.1 (Educational Project; contact@example.com)'
            )
            
            # Navegar y esperar a que cargue
            page.goto(url, wait_until='networkidle', timeout=timeout)
            
            # Esperar un poco m√°s para JS as√≠ncrono
            page.wait_for_timeout(2000)  # 2 segundos adicionales
            
            html = page.content()
            browser.close()
            
            logger.info(f"‚úì HTML din√°mico obtenido: {len(html)} caracteres")
            return html
            
    except PlaywrightTimeout:
        logger.error(f"‚úó Timeout en navegador headless para {url}")
        return None
        
    except Exception as e:
        logger.error(f"‚úó Error en Playwright: {e}")
        return None


def smart_scrape(url: str, force_dynamic: bool = False) -> Tuple[Optional[str], List[str], str]:
    """
    Scraping inteligente: intenta est√°tico primero, fallback a din√°mico si es necesario.
    
    Args:
        url: URL a scrapear
        force_dynamic: Si True, usa Playwright directamente
        
    Returns:
        (html, enlaces, m√©todo_usado)
        m√©todo_usado: 'static' o 'dynamic'
    """
    # Rate limiting
    time.sleep(RATE_LIMIT_DELAY)
    
    # Si se fuerza din√°mico, ir directo a Playwright
    if force_dynamic:
        logger.info("‚ö° Modo din√°mico forzado")
        html = fetch_html_dynamic(url)
        if html:
            links = extract_links(html, url)
            return html, links, 'dynamic'
        else:
            return None, [], 'dynamic'
    
    # Intentar est√°tico primero
    logger.info("üìÑ Intentando scraping est√°tico...")
    html = fetch_html(url)
    
    if html is None:
        logger.warning("Scraping est√°tico fall√≥, intentando din√°mico...")
        html = fetch_html_dynamic(url)
        method = 'dynamic'
    elif is_incomplete_html(html, url):
        logger.warning("HTML incompleto detectado, cambiando a scraping din√°mico...")
        html = fetch_html_dynamic(url)
        method = 'dynamic'
    else:
        method = 'static'
    
    if html is None:
        return None, [], method
    
    links = extract_links(html, url)
    
    logger.info(f"‚úÖ Scraping completado con m√©todo: {method}")
    return html, links, method